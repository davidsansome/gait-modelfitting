\section{Conclusion}

From Table \ref{ClassificationResults2} we can see that there is one winning combination in terms of correct classification rates.
This combination is summarised in Table \ref{ConclusionTable}.

\begin{table}[hb]
	\centering
	\begin{tabular}{r|l}
		Search resolution & res$_\theta$ = 21, res$_\alpha$ = 11 \\
		Multiresolution search? & No \\
		Parameters to include in the DFT & Both thigh $\theta$ values\\
		Number of DFT components to use & All \\
		Additional classifiers & Subject's height \\
		Normalise for mean? & Yes \\
		Normalise for variance? & No \\
		Distance measure & Euclidean distance in complex plane
	\end{tabular}
	\caption{Summary of the winning algorithm.}
	\label{ConclusionTable}
\end{table}

One interesting point regarding the results is that a higher search resolution, and correspondingly a closer fit to the manual fitting, did not result in a higher classification rate.
Intuition would suggest that as the resolution of our search increased the ``quality'' of the result would improve and the algorithm would be able to identify the subject with more accuracy.
In fact the opposite occurred - the best performers at classification were consistently the ones with the lowest search resolution.

A tentative explanation for this is that as resolution of the search increases, the fitting process picks up more of the small high frequency variations and noise in the voxel data.
This in turn provides the DFT with extra irrelevant information, corrupting the results.

\bigskip
\noindent Our best algorithm gives a 90\% success rate when applied to our dataset.
The two samples that were classified incorrectly in this test (4a and 4b) were actually very close to receiving the correct classification - with only one or two other samples in between them and a sample of the correct class.

Previous research into gait recognition has typically produced algorithms that have success rates between 90\% and 100\%.
It is difficult to compare these figures directly however for a number of reasons:

\begin{enumerate}
	\item Other pieces of research sometimes use a larger number of training points.
		In our tests we took two of the four samples for our training set, and used the remaining two to test the classification process.
		However in \cite{GaitModels} Cunado et al.\ used three samples in the training set, and only one for classification.
		Increasing the size of the training set in this way will obviously change the success rate.
	\item The database is so small (10 subjects), that any differences in classification rate between algorithms below 10\% are probably meaningless anyway.
\end{enumerate}

The only definite conclusion we can take from our testing is that our results are at least as good as those in previous research.

\bigskip
\noindent An important question to ask is whether model fitting in 3D produces better results than model fitting in 2D.
This is the first work in 3D model fitting and as such there are very few existing comparisons with 2D systems.
It would be interesting to evaluate the performance of some 2D algorithms by taking planar side views of the 3D videos in our dataset.
However sadly there was insufficient time to perform this comparison.

Despite this our low error rates in Table \ref{ManualFitTable} and high classification rates in Table \ref{ClassificationResults2} are both very encouraging at the present.
