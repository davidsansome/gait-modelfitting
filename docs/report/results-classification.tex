\section{Accuracy of classification}
\label{Results:Classification}

In the previous chapters we have presented a selection of different algorithms and approaches to modelfitting and classification.
This section attempts to discover which combination of these algorithms leads to the best performance in terms of recognising new subjects.

The sample data used to test the algorithms contains 10 different subjects.
Four recordings of each subject were taken, giving a total of 40 samples.
Two of the four samples from each subject were manually classified and entered into a database for use as a training set,
while the remaining two samples (labelled \emph{a} and \emph{b}) were put through automatic classification.

The success rate of the system was determined by how many of the ``unknown'' samples \emph{a} and \emph{b} it was able to match to the correct class in the training set.
In the tables below (\ref{ClassificationResults} and \ref{ClassificationResults2}) the numbers in each cell represent how close the algorithm came to correctly identifying the subject.
A value of 3 would indicate that there were 3 other classes that were a better match than the correct one,
while a value of 0 would represent a correct classification.

\bigskip
\noindent There were four primary questions that guided the testing process:

\begin{enumerate}
	\item \textbf{Which distance function is most suitable?}
		Section \ref{ClassificationMethods} described some of the possible \emph{distance functions} that could be implemented to find how closely related two samples are to each other.
		These functions take the results of the DFT as applied to both of the samples and compare each of the frequency components.
		Table \ref{ClassificationResults} over the page shows the performance of each of these functions.
		
		First we test the basic distance functions.
		Four such functions are tested - one that compares only the magnitudes of the frequency components, one that compares the phase weighted magnitudes, and two that take the polar and Euclidean distances between two frequency components in the complex plane.
		From the results we can see that the highest performers are the polar and Euclidean distance functions.
		The low performance of the phase-weighted magnitude function can be explained by the fact that it does not take into account the modular nature of the phase.
		As explained in Section \ref{ClassificationMethods} a comparison of two phase values $+\frac{5}{6}\pi$ and $-\frac{5}{6}\pi$ would incorrectly produce an answer of $\frac{10}{6}\pi$.
		
		Next all the functions are re-tested with the first frequency component of each sample being excluded.
		The results show that this actually lowers the performance of all the functions - suggesting that the first frequency component might contain useful identifying information.
		
		The Euclidean distance function is then modified to normalise the mean of the samples' DFT results.
		This does indeed improve the performance - increasing the correct classification rate by 5\%.
	
	\item \textbf{Which of the search algorithms are best?}
		We started off by introducing a fixed resolution search in Listing \ref{leastsquarescode}, and then later attempted to lower its execution time by splitting it into two passes.
		It is important to evaluate both these fixed and multi-resolution search algorithms in order to ensure that we are not sacrificing performance for speed of execution.
		
		Three algorithms were tested - a fixed resolution search with a low resolution ($\text{res}_\alpha = 11, \text{res}_\theta = 21$),
		a fixed resolution search with a high resolution ($\text{res}_\alpha = 11, \text{res}_\theta = 41$),
		and a multi-resolution search (with equivalent $\text{res}_\alpha = 41, \text{res}_\theta = 81$).
		The results from these tests are shown in Table \ref{ClassificationResults2}.
		
		Surprisingly, these tests show that the best performer is the low fixed-resolution search algorithm.
		Using this algorithm we can correctly classify 80\% of all the test subjects.
		Both the high resolution and multi resolution algorithms seem to have much lower performance - demonstrating a lower overall classification rate.
		However if we look at the distances between our sample and the nearest sample of the correct class we can see that they are still very close - the higher resolution searches
		have just moved a sample of another class a little bit closer, throwing off the classification.
		
	\item \textbf{Which limbs are important in classification?}
		Our initial tests in Table \ref{ClassificationResults} were performed using only the signature data from the subject's left thigh.
		In Table \ref{ClassificationResults2} we run additional tests to see if there is any improvement to be gained from including the right thigh and lower legs.
		
		Using both thighs in classification gives an improvement over just using the left thigh - one additional sample is classified correctly bringing the success rate up from 75\% to 80\%.
		
		Including the lower legs brings about a drop in performance of 10\%-20\% over all the search algorithms, so clearly these are less useful than the thighs for identification.
		Using \emph{both} thighs and lower legs seems to bring us to a middle ground in classification performance - 65\% of the samples being classified correctly.
	
	\item \textbf{Can the subject's height be used for classification?}
		The final test performed was to find out whether including the subject's height as an additional classifier would boost performance.
		
		Figure \ref{ClassificationResults2} shows that including the height provides a huge boost in performance, bringing the overall success rate up to 90\%.
		Interestingly this was the only test that correctly classified Sample 10b.
\end{enumerate}

\begin{landscape}
	\begin{table}[p]
		\centering
		\begin{tabular}{|l|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c|}
			\hline
			& \multicolumn{20}{|c|}{Sample} & Success rate \\
			& 1a & 1b & 2a & 2b & 3a & 3b & 4a & 4b & 5a & 5b & 6a & 6b & 7a & 7b & 8a & 8b & 9a & 9b & 10a & 10b & \\
			
			\hline
			Magnitude                  & 1 & 2 & 1 & 0 & 0 & 1 & 6 & 11 & 4 & 0 & 3 & 0 & 0 & 2 & 0 & 0 & 1 & 2 & 2 & 0 & 40\% \\
			Phase-mag                  & 0 & 2 & 0 & 6 & 0 & 1 & 12 & 10 & 3 & 3 & 0 & 4 & 1 & 0 & 0 & 2 & 10 & 16 & 1 & 5 & 30\% \\
			Polar dist                 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 11 & 0 & 3 & 2 & 0 & 0 & 0 & 0 & 0 & 7 & 0 & 0 & 16 & 70\% \\
			Eucl dist                  & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 11 & 0 & 3 & 2 & 0 & 0 & 0 & 0 & 0 & 7 & 0 & 0 & 16 & 70\% \\
			
			\hline
			Mag excl first             & 2 & 1 & 1 & 6 & 1 & 1 & 0 & 2 & 14 & 2 & 3 & 0 & 1 & 8 & 0 & 7 & 1 & 2 & 4 & 1 & 15\% \\
			Phase-mag excl first       & 0 & 2 & 0 & 6 & 0 & 1 & 12 & 12 & 3 & 3 & 0 & 2 & 1 & 0 & 0 & 2 & 10 & 2 & 1 & 15 & 30\% \\
			Polar dist excl first      & 0 & 2 & 0 & 2 & 0 & 0 & 0 & 11 & 0 & 5 & 3 & 0 & 0 & 0 & 0 & 2 & 9 & 0 & 2 & 16 & 55\% \\
			Eucl dist excl first       & 0 & 2 & 0 & 2 & 0 & 0 & 0 & 11 & 0 & 5 & 3 & 0 & 0 & 0 & 0 & 2 & 9 & 0 & 2 & 16 & 55\% \\
			
			\hline
			Eucl dist norm             & 0 & 2 & 0 & 0 & 0 & 0 & 0 & 11 & 0 & 0 & 4 & 0 & 0 & 0 & 0 & 0 & 7 & 0 & 0 & 16 & 75\% \\
			
			\hline
		\end{tabular}
		\caption{Comparison of different implementations of the distanceTo() function.}
		\label{ClassificationResults}
	\end{table}
	
	\begin{table}[p]
		\centering
		\begin{tabular}{|l|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c@{ }c|c|}
			\hline
			& \multicolumn{20}{|c|}{Sample} & Success rate \\
			& 1a & 1b & 2a & 2b & 3a & 3b & 4a & 4b & 5a & 5b & 6a & 6b & 7a & 7b & 8a & 8b & 9a & 9b & 10a & 10b & \\
			
			\hline
			\multicolumn{22}{|l|}{\textbf{Using both thigh $\theta$ values.}} \\
			Low res search             & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 4 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 18 & 80\% \\
			High res search            & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 5 & 0 & 1 & 2 & 0 & 0 & 0 & 0 & 0 & 8 & 0 & 0 & 18 & 65\% \\
			Multi res search           & 0 & 1 & 0 & 0 & 0 & 0 & 2 & 2 & 0 & 1 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 18 & 60\% \\
			
			\hline
			\multicolumn{22}{|l|}{\textbf{Using both lower leg $\theta$ values.}} \\
			Low res search             & 0 & 0 & 0 & 2 & 0 & 1 & 2 & 0 & 1 & 5 & 0 & 1 & 0 & 0 & 0 & 4 & 0 & 0 & 0 & 18 & 60\% \\
			High res search            & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 2 & 1 & 5 & 1 & 2 & 0 & 0 & 1 & 3 & 4 & 1 & 0 & 17 & 45\% \\
			Multi res search           & 0 & 2 & 0 & 0 & 0 & 0 & 4 & 4 & 2 & 5 & 1 & 1 & 0 & 0 & 0 & 0 & 4 & 2 & 0 & 17 & 50\% \\
			
			\hline
			\multicolumn{22}{|l|}{\textbf{Using both thigh and lower leg $\theta$ values.}} \\
			Low res search             & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 2 & 0 & 3 & 0 & 1 & 0 & 0 & 0 & 0 & 3 & 0 & 0 & 18 & 65\% \\
			High res search            & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 2 & 0 & 4 & 2 & 1 & 0 & 0 & 0 & 0 & 4 & 0 & 0 & 18 & 65\% \\
			Multi res search           & 0 & 1 & 0 & 0 & 0 & 0 & 3 & 3 & 0 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 3 & 1 & 0 & 18 & 65\% \\
			
			\hline
			\multicolumn{22}{|l|}{\textbf{Including the subject's height as an additional classifier}} \\
			Low res search             & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 90\% \\
			
			\hline
		\end{tabular}
		\caption{Comparison of different modelfitting algorithms and parameters.
			These tests all use the mean-normalising Euclidean distance function from the previous page.}
		\label{ClassificationResults2}
	\end{table}
\end{landscape}

